import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import time
import tracemalloc
import os

def pandas_version(filepath, output_path):
    """Original pandas implementation for comparison"""
    tracemalloc.start()
    start_time = time.time()
    
    records = []
    with open(filepath, 'r') as file:
        line_count = int(file.readline().strip())
        for i in range(line_count):
            parts = file.readline().strip().split()
            entry_type = parts[0]
            tag_count = int(parts[1])
            tags = parts[2:2 + tag_count]
            records.append({
                "id": i + 1,
                "type": "Landscape" if entry_type == 'L' else "Portrait",
                "tag_count": tag_count,
                "tags": tags
            })

    df = pd.DataFrame(records)
    df.to_parquet(output_path, engine='pyarrow')
    
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    return {
        "time": time.time() - start_time,
        "peak_memory": peak / (1024**2),
        "approach": "Pandas"
    }

def optimized_arrow(filepath, output_path):
    """Optimized PyArrow implementation"""
    tracemalloc.start()
    start_time = time.time()
    
    # Initialize builders with explicit types
    schema = pa.schema([
        ('id', pa.int32()),
        ('type', pa.string()),
        ('tag_count', pa.int32()),
        ('tags', pa.list_(pa.string()))
    ])
    writer = pq.ParquetWriter(
        output_path,
        schema,
        compression='SNAPPY',
        version='2.6'
    )
    
    batch_size = 10000
    current_batch = {
        'id': [],
        'type': [],
        'tag_count': [],
        'tags': []
    }
    
    with open(filepath, 'r') as file:
        line_count = int(file.readline().strip())
        
        for i in range(line_count):
            parts = file.readline().strip().split()
            entry_type = parts[0]
            tag_count = int(parts[1])
            
            current_batch['id'].append(i + 1)
            current_batch['type'].append(
                "Landscape" if entry_type == 'L' else "Portrait"
            )
            current_batch['tag_count'].append(tag_count)
            current_batch['tags'].append(parts[2:2 + tag_count])
            
            if len(current_batch['id']) >= batch_size:
                table = pa.Table.from_pydict(current_batch, schema=schema)
                writer.write_table(table)
                current_batch = {k: [] for k in current_batch}  # Reset batch
    
    # Write remaining records
    if current_batch['id']:
        table = pa.Table.from_pydict(current_batch, schema=schema)
        writer.write_table(table)
    
    writer.close()
    
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    return {
        "time": time.time() - start_time,
        "peak_memory": peak / (1024**2),
        "approach": "PyArrow"
    }

def main():
    input_file = ".\\Data\\1_binary_landscapes.txt"  # Replace with your input file
    output_pandas = "output_pandas.parquet"
    output_arrow = "output_arrow.parquet"
    
    # Run both versions
    results = []
    
    print("Running Pandas version...")
    results.append(pandas_version(input_file, output_pandas))
    
    print("Running PyArrow version...")
    results.append(optimized_arrow(input_file, output_arrow))
    
    # Compare file sizes
    pandas_size = os.path.getsize(output_pandas) / (1024**2)
    arrow_size = os.path.getsize(output_arrow) / (1024**2)
    
    print("\nPerformance Comparison:")
    print(f"{'Approach':<10} | {'Time (s)':<10} | {'Peak Memory (MB)':<20} | {'Output Size (MB)':<15}")
    print("-" * 65)
    for r in results:
        print(f"{r['approach']:<10} | {r['time']:<10.4f} | {r['peak_memory']:<20.2f} | ", end="")
        print(f"{pandas_size if r['approach'] == 'Pandas' else arrow_size:<15.2f}")

if __name__ == "__main__":
    main()
